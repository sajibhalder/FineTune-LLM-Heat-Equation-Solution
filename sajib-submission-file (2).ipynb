{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91328,"databundleVersionId":10789290,"sourceType":"competition"}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##   Finetune LLM-Interpret Heat Equation Splution\n#### Github code and Documentation:\n\n-    https://github.com/sajibhalder/FineTune-LLM-Heat-Equation-Solution","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:15:40.900470Z","iopub.execute_input":"2025-01-23T18:15:40.900692Z","iopub.status.idle":"2025-01-23T18:15:41.501040Z","shell.execute_reply.started":"2025-01-23T18:15:40.900657Z","shell.execute_reply":"2025-01-23T18:15:41.500051Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/fine-tuning-lm-physical-interpretation-hackathon/Sample_submission.csv\n/kaggle/input/fine-tuning-lm-physical-interpretation-hackathon/Case3.vtk\n/kaggle/input/fine-tuning-lm-physical-interpretation-hackathon/Case4.vtk\n/kaggle/input/fine-tuning-lm-physical-interpretation-hackathon/Case1.vtk\n/kaggle/input/fine-tuning-lm-physical-interpretation-hackathon/Case2.vtk\n/kaggle/input/fine-tuning-lm-physical-interpretation-hackathon/Questions.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 1. Dataset Generation for Heat Equation Solution- Case1, Case2, Case3, Case4\nThe time dependent heat equation is given as\n## ∂T∂t−α(∂2∂x2+∂2∂y2)T=f(x,y,t)\nwhere\n\nT\n : Temperature field\nα\n : Thermal conductivity\nf\n : Force function\nDetails of the solution for different cases on [0,1] X [0,1] sqaure :\n\n## Case1 : ∂²T/∂x² + ∂²T/∂y² = 8π²sin(2πx)sin(2πy)\ncontent_copy\nBoundary Conditions\n\nT(0,y) = 0\nT(1,y) = 0\nT(x,0) = 0\nT(x,1) = 0\n\n## Case2 : ∂²T/∂x² + ∂²T/∂y² = 0  \ncontent_copy\nInitial and Boundary Conditions\n\nT(0,y) = 0\nT(1,y) = 0\nT(x,0) = 0\nT(x,1) = sin(πx)\n\n## Case3 : ∂²T/∂x² + ∂²T/∂y² = 0\ncontent_copy\nBoundary Conditions\n\nT(0,y) = 0\nT(1,y) = y(1-y)\nT(x,0) = 0\nT(x,1) = 0\n\n## Case4 : ∂T/∂t = α(∂²T/∂x² + ∂²T/∂y²)\ncontent_copy\nInitial and Boundary Conditions\n\nT(x,y,0) = sin(πx)sin(πy)\nT(0,y,t) = 0\nT(1,y,t) = 0\nT(x,0,t) = 0\nT(x,1,t) = 0\nα = 0.01","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Parameters\nL = 1.0  # Length of the domain\nN = 40  # Number of grid points (adjusted for max 10k rows dataset size)\nalpha = 0.01  # Thermal conductivity\nT_max = 1.0  # Maximum time\ndt = 0.001  # Time step\ndx = dy = L / N  # Grid spacing\n\n# Discretized spatial domain\nx = np.linspace(0, L, N)\ny = np.linspace(0, L, N)\nX, Y = np.meshgrid(x, y)\n\n# Initialize temperature field\nT = np.zeros((N, N))\n\n# Define the source term f(x, y)\ndef source_term(x, y):\n    return 8 * np.pi**2 * np.sin(2 * np.pi * x) * np.sin(2 * np.pi * y)\n\n# Time loop for solving the heat equation\ndef solve_heat_equation_case_1(T, alpha, dt, dx, dy, T_max, max_rows=10000):\n    num_time_steps = int(T_max / dt)\n    dataset = []\n    \n    # Apply boundary conditions\n    for i in range(N):\n        for j in range(N):\n            if i == 0 or i == N-1 or j == 0 or j == N-1:\n                T[i, j] = 0  # Boundary conditions set to 0 as per the problem\n    \n    time_steps_taken = 0  # To ensure dataset doesn't exceed max_rows\n    \n    # Time-stepping loop\n    for t in range(num_time_steps):\n        T_new = T.copy()\n        \n        # Interior points update (finite difference method)\n        for i in range(1, N-1):\n            for j in range(1, N-1):\n                T_new[i, j] = T[i, j] + alpha * dt * (\n                    (T[i+1, j] - 2*T[i, j] + T[i-1, j]) / dx**2 +\n                    (T[i, j+1] - 2*T[i, j] + T[i, j-1]) / dy**2 +\n                    source_term(x[i], y[j])\n                )\n        \n        T = T_new\n        \n        # Save data for each time step, but limit dataset size\n        for i in range(N):\n            for j in range(N):\n                if len(dataset) < max_rows:\n                    dataset.append([x[i], y[j], T[i, j], t*dt])\n                if len(dataset) >= max_rows:\n                    break\n            if len(dataset) >= max_rows:\n                break\n        if len(dataset) >= max_rows:\n            break\n    \n    return pd.DataFrame(dataset, columns=['x', 'y', 'z', 'Temperature'])\n\n# Solve for Case 1\ndataset_case_1 = solve_heat_equation_case_1(T=T, alpha=alpha, dt=dt, dx=dx, dy=dy, T_max=T_max)\n\n\n# Save as CSV\ncsv_file_path = \"heat_eq_case_1.csv\"\ndataset_case_1.to_csv(csv_file_path, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:15:49.017389Z","iopub.execute_input":"2025-01-23T18:15:49.017722Z","iopub.status.idle":"2025-01-23T18:15:49.170189Z","shell.execute_reply.started":"2025-01-23T18:15:49.017690Z","shell.execute_reply":"2025-01-23T18:15:49.169541Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Parameters\nL = 1.0  # Length of the domain\nN = 40  # Number of grid points (adjusted for max 10k rows dataset size)\nalpha = 0.01  # Thermal conductivity (this doesn't affect Case 2 as equation is homogeneous)\nT_max = 1.0  # Maximum time\ndt = 0.001  # Time step\ndx = dy = L / N  # Grid spacing\n\n# Discretized spatial domain\nx = np.linspace(0, L, N)\ny = np.linspace(0, L, N)\nX, Y = np.meshgrid(x, y)\n\n# Initialize temperature field\nT = np.zeros((N, N))\n\n# Boundary conditions for Case 2\ndef boundary_conditions(x, y):\n    # T(x,0) = 0, T(x,1) = sin(πx), T(0,y) = 0, T(1,y) = 0\n    if y == 0 or y == 1:\n        return 0\n    if x == 0 or x == 1:\n        return 0\n    return np.sin(np.pi * x)  # For y=1, the boundary condition is sin(πx)\n\n# Time loop for solving the heat equation\ndef solve_heat_equation_case_2(T, alpha, dt, dx, dy, T_max, max_rows=10000):\n    num_time_steps = int(T_max / dt)\n    dataset = []\n    \n    # Apply initial and boundary conditions\n    for i in range(N):\n        for j in range(N):\n            T[i, j] = boundary_conditions(x[i], y[j])\n    \n    time_steps_taken = 0  # To ensure dataset doesn't exceed max_rows\n    \n    # Time-stepping loop\n    for t in range(num_time_steps):\n        T_new = T.copy()\n        \n        # Interior points update (finite difference method)\n        for i in range(1, N-1):\n            for j in range(1, N-1):\n                T_new[i, j] = T[i, j] + alpha * dt * (\n                    (T[i+1, j] - 2*T[i, j] + T[i-1, j]) / dx**2 +\n                    (T[i, j+1] - 2*T[i, j] + T[i, j-1]) / dy**2\n                )\n        \n        T = T_new\n        \n        # Save data for each time step, but limit dataset size\n        for i in range(N):\n            for j in range(N):\n                if len(dataset) < max_rows:\n                    dataset.append([x[i], y[j], T[i, j], t*dt])\n                if len(dataset) >= max_rows:\n                    break\n            if len(dataset) >= max_rows:\n                break\n        if len(dataset) >= max_rows:\n            break\n    \n    return pd.DataFrame(dataset, columns=['x', 'y', 'z', 'Temperature'])\n\n# Solve for Case 2\ndataset_case_2 = solve_heat_equation_case_2(T=T, alpha=alpha, dt=dt, dx=dx, dy=dy, T_max=T_max)\n\n\n# Save as CSV\ncsv_file_path = \"heat_eq_case_2.csv\"\ndataset_case_2.to_csv(csv_file_path, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:15:55.743256Z","iopub.execute_input":"2025-01-23T18:15:55.743544Z","iopub.status.idle":"2025-01-23T18:15:55.835429Z","shell.execute_reply.started":"2025-01-23T18:15:55.743520Z","shell.execute_reply":"2025-01-23T18:15:55.834829Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Parameters\nL = 1.0  # Length of the domain\nN = 40  # Number of grid points (adjusted for max 10k rows dataset size)\nalpha = 0.01  # Thermal conductivity (not required here as it's a steady-state equation)\nT_max = 1.0  # Maximum time\ndt = 0.001  # Time step\ndx = dy = L / N  # Grid spacing\n\n# Discretized spatial domain\nx = np.linspace(0, L, N)\ny = np.linspace(0, L, N)\nX, Y = np.meshgrid(x, y)\n\n# Initialize temperature field\nT = np.zeros((N, N))\n\n# Boundary conditions for Case 3\ndef boundary_conditions(x, y):\n    if x == 0:\n        return 0  # T(0, y) = 0\n    if x == 1:\n        return y * (1 - y)  # T(1, y) = y(1 - y)\n    if y == 0 or y == 1:\n        return 0  # T(x,0) = 0 and T(x,1) = 0\n    return None  # Interior points\n\n# Apply boundary conditions\nfor i in range(N):\n    for j in range(N):\n        bc_value = boundary_conditions(x[i], y[j])\n        if bc_value is not None:\n            T[i, j] = bc_value\n\n# Solve the heat equation using the finite difference method\ndef solve_heat_equation_case_3(T, alpha, dt, dx, dy, T_max, max_rows=10000):\n    num_time_steps = int(T_max / dt)\n    dataset = []\n\n    time_steps_taken = 0  # To ensure dataset doesn't exceed max_rows\n\n    for t in range(num_time_steps):\n        T_new = T.copy()\n\n        # Interior points update (finite difference method)\n        for i in range(1, N-1):\n            for j in range(1, N-1):\n                T_new[i, j] = T[i, j] + alpha * dt * (\n                    (T[i+1, j] - 2*T[i, j] + T[i-1, j]) / dx**2 +\n                    (T[i, j+1] - 2*T[i, j] + T[i, j-1]) / dy**2\n                )\n\n        T = T_new\n\n        # Save data for each time step, but limit dataset size\n        for i in range(N):\n            for j in range(N):\n                if len(dataset) < max_rows:\n                    dataset.append([x[i], y[j], T[i, j], t*dt])\n                if len(dataset) >= max_rows:\n                    break\n            if len(dataset) >= max_rows:\n                break\n        if len(dataset) >= max_rows:\n            break\n\n    return pd.DataFrame(dataset, columns=['x', 'y', 'z', 'Temperature'])\n\n# Solve for Case 3\ndataset_case_3 = solve_heat_equation_case_3(T=T, alpha=alpha, dt=dt, dx=dx, dy=dy, T_max=T_max)\n\n# Save as CSV\ncsv_file_path = \"heat_eq_case_3.csv\"\ndataset_case_3.to_csv(csv_file_path, index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:16:02.111315Z","iopub.execute_input":"2025-01-23T18:16:02.111632Z","iopub.status.idle":"2025-01-23T18:16:02.192736Z","shell.execute_reply.started":"2025-01-23T18:16:02.111608Z","shell.execute_reply":"2025-01-23T18:16:02.192054Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Parameters\nL = 1.0  # Length of the domain\nN = 40  # Number of grid points (adjusted for max 10k rows dataset size)\nalpha = 0.01  # Thermal diffusivity\nT_max = 1.0  # Maximum simulation time\ndt = 0.001  # Time step\ndx = dy = L / N  # Grid spacing\n\n# Discretized spatial domain\nx = np.linspace(0, L, N)\ny = np.linspace(0, L, N)\nX, Y = np.meshgrid(x, y)\n\n# Initialize temperature field with initial condition T(x,y,0) = sin(πx)sin(πy)\nT = np.sin(np.pi * X) * np.sin(np.pi * Y)\n\n# Boundary conditions for Case 4\ndef apply_boundary_conditions(T):\n    T[0, :] = 0  # T(0,y,t) = 0\n    T[-1, :] = 0  # T(1,y,t) = 0\n    T[:, 0] = 0  # T(x,0,t) = 0\n    T[:, -1] = 0  # T(x,1,t) = 0\n    return T\n\n# Solve the heat equation using the finite difference method\ndef solve_heat_equation_case_4(T, alpha, dt, dx, dy, T_max, max_rows=10000):\n    num_time_steps = int(T_max / dt)\n    dataset = []\n\n    for t in range(num_time_steps):\n        T_new = T.copy()\n\n        # Interior points update (explicit finite difference method)\n        for i in range(1, N-1):\n            for j in range(1, N-1):\n                T_new[i, j] = T[i, j] + alpha * dt * (\n                    (T[i+1, j] - 2*T[i, j] + T[i-1, j]) / dx**2 +\n                    (T[i, j+1] - 2*T[i, j] + T[i, j-1]) / dy**2\n                )\n\n        T = apply_boundary_conditions(T_new)  # Apply boundary conditions\n\n        # Save data for each time step, but limit dataset size\n        for i in range(N):\n            for j in range(N):\n                if len(dataset) < max_rows:\n                    dataset.append([x[i], y[j], T[i, j], t * dt])\n                if len(dataset) >= max_rows:\n                    break\n            if len(dataset) >= max_rows:\n                break\n        if len(dataset) >= max_rows:\n            break\n\n    return pd.DataFrame(dataset, columns=['x', 'y', 'z', 'Temperature'])\n\n# Solve for Case 4\ndataset_case_4 = solve_heat_equation_case_4(T=T, alpha=alpha, dt=dt, dx=dx, dy=dy, T_max=T_max)\n\n# Save to CSV\ncsv_file_path = \"heat_eq_case_4.csv\"\ndataset_case_4.to_csv(csv_file_path, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:16:08.110425Z","iopub.execute_input":"2025-01-23T18:16:08.110817Z","iopub.status.idle":"2025-01-23T18:16:08.246420Z","shell.execute_reply.started":"2025-01-23T18:16:08.110786Z","shell.execute_reply":"2025-01-23T18:16:08.245822Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"csv_files = [\"/kaggle/working/heat_eq_case_1.csv\",\n                \"/kaggle/working/heat_eq_case_2.csv\",\n                \"/kaggle/working/heat_eq_case_3.csv\",\n                \"/kaggle/working/heat_eq_case_4.csv\"]\n# Merge all CSV files\nfinal_df = pd.concat([pd.read_csv(csv) for csv in csv_files], ignore_index=True)\n\n# Save the final merged dataset\nfinal_csv_path = \"/kaggle/working/final_heat_eq.csv\"\nfinal_df.to_csv(final_csv_path, index=False)\n\nprint(f\"Final dataset saved: {final_csv_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:16:14.246450Z","iopub.execute_input":"2025-01-23T18:16:14.246893Z","iopub.status.idle":"2025-01-23T18:16:14.507561Z","shell.execute_reply.started":"2025-01-23T18:16:14.246855Z","shell.execute_reply":"2025-01-23T18:16:14.506714Z"}},"outputs":[{"name":"stdout","text":"Final dataset saved: /kaggle/working/final_heat_eq.csv\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\ndata_path = \"/kaggle/working/final_heat_eq.csv\"  # Update with your dataset name\ndf = pd.read_csv(data_path)\n\n# Display the first few rows\nprint(df.head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:16:19.614451Z","iopub.execute_input":"2025-01-23T18:16:19.614800Z","iopub.status.idle":"2025-01-23T18:16:19.660160Z","shell.execute_reply.started":"2025-01-23T18:16:19.614771Z","shell.execute_reply":"2025-01-23T18:16:19.659190Z"}},"outputs":[{"name":"stdout","text":"     x         y    z  Temperature\n0  0.0  0.000000  0.0          0.0\n1  0.0  0.025641  0.0          0.0\n2  0.0  0.051282  0.0          0.0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Create LLM fine-tuning dataset\ndf[\"prompt\"] = df.apply(lambda row: \n    f\"What is the temperature at point ({row['x']}, {row['y']}, {row['z']})?\", axis=1)\ndf[\"response\"] = df[\"Temperature\"].apply(lambda temp: f\"The temperature is {temp}.\")\n\n# Save the fine-tuning dataset\ndf[[\"prompt\", \"response\"]].to_csv(\"heat_eq_finetune.csv\", index=False)\nprint(\"Fine-tuning CSV saved: heat_eq_finetune.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:16:24.692059Z","iopub.execute_input":"2025-01-23T18:16:24.692476Z","iopub.status.idle":"2025-01-23T18:16:25.292179Z","shell.execute_reply.started":"2025-01-23T18:16:24.692437Z","shell.execute_reply":"2025-01-23T18:16:25.291186Z"}},"outputs":[{"name":"stdout","text":"Fine-tuning CSV saved: heat_eq_finetune.csv\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\ndata_path = \"/kaggle/working/heat_eq_finetune.csv\"  # Update with your dataset name\ndf = pd.read_csv(data_path)\n\n# Display the first few rows\nprint(df.head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:16:30.298878Z","iopub.execute_input":"2025-01-23T18:16:30.299188Z","iopub.status.idle":"2025-01-23T18:16:30.359326Z","shell.execute_reply.started":"2025-01-23T18:16:30.299166Z","shell.execute_reply":"2025-01-23T18:16:30.358585Z"}},"outputs":[{"name":"stdout","text":"                                              prompt                 response\n0  What is the temperature at point (0.0, 0.0, 0.0)?  The temperature is 0.0.\n1  What is the temperature at point (0.0, 0.02564...  The temperature is 0.0.\n2  What is the temperature at point (0.0, 0.05128...  The temperature is 0.0.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 2. Data Scraping and Dataset Generation from given VTK Input file","metadata":{}},{"cell_type":"code","source":"!pip install pyvista","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:16:40.206901Z","iopub.execute_input":"2025-01-23T18:16:40.207186Z","iopub.status.idle":"2025-01-23T18:16:45.228338Z","shell.execute_reply.started":"2025-01-23T18:16:40.207166Z","shell.execute_reply":"2025-01-23T18:16:45.227502Z"}},"outputs":[{"name":"stdout","text":"Collecting pyvista\n  Downloading pyvista-0.44.2-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: matplotlib>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from pyvista) (3.7.5)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pyvista) (1.26.4)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pyvista) (11.0.0)\nRequirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from pyvista) (1.8.2)\nRequirement already satisfied: scooby>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from pyvista) (0.10.0)\nRequirement already satisfied: vtk<9.4.0 in /usr/local/lib/python3.10/dist-packages (from pyvista) (9.3.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyvista) (4.12.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.1->pyvista) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.1->pyvista) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.1->pyvista) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.1->pyvista) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.1->pyvista) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.1->pyvista) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.1->pyvista) (2.8.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->pyvista) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->pyvista) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->pyvista) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->pyvista) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->pyvista) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->pyvista) (2.4.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->pyvista) (4.3.6)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch->pyvista) (2.32.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.1->pyvista) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->pyvista) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->pyvista) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->pyvista) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->pyvista) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.0->pyvista) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.0->pyvista) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->pyvista) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.0->pyvista) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.0->pyvista) (2024.2.0)\nDownloading pyvista-0.44.2-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyvista\nSuccessfully installed pyvista-0.44.2\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import pyvista as pv\n\n# Load the VTK file\nvtk_file_path = \"/kaggle/input/fine-tuning-lm-physical-interpretation-hackathon/Case1.vtk\"\nmesh = pv.read(vtk_file_path)\n\n# Print information about the mesh\nprint(mesh)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:16:55.943355Z","iopub.execute_input":"2025-01-23T18:16:55.943703Z","iopub.status.idle":"2025-01-23T18:16:56.830305Z","shell.execute_reply.started":"2025-01-23T18:16:55.943675Z","shell.execute_reply":"2025-01-23T18:16:56.829605Z"}},"outputs":[{"name":"stdout","text":"StructuredGrid (0x796ec6b44d60)\n  N Cells:      9801\n  N Points:     10000\n  X Bounds:     0.000e+00, 1.000e+00\n  Y Bounds:     0.000e+00, 1.000e+00\n  Z Bounds:     0.000e+00, 0.000e+00\n  Dimensions:   100, 100, 1\n  N Arrays:     1\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import numpy as np\n\n# Convert VTK points to NumPy array\npoints = np.array(mesh.points)\n\n# Convert cell data to NumPy\ncell_data = mesh.cell_data\npoint_data = mesh.point_data\n\nprint(\"Points:\\n\", points)\nprint(\"Point Data:\\n\", point_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:17:01.322602Z","iopub.execute_input":"2025-01-23T18:17:01.322921Z","iopub.status.idle":"2025-01-23T18:17:01.330453Z","shell.execute_reply.started":"2025-01-23T18:17:01.322899Z","shell.execute_reply":"2025-01-23T18:17:01.329727Z"}},"outputs":[{"name":"stdout","text":"Points:\n [[0.         0.         0.        ]\n [0.         0.01010101 0.        ]\n [0.         0.02020202 0.        ]\n ...\n [1.         0.97979798 0.        ]\n [1.         0.98989899 0.        ]\n [1.         1.         0.        ]]\nPoint Data:\n pyvista DataSetAttributes\nAssociation     : POINT\nActive Scalars  : Temperature\nActive Vectors  : None\nActive Texture  : None\nActive Normals  : None\nContains arrays :\n    Temperature             float64    (10000,)             SCALARS\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pyvista as pv\nimport pandas as pd\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:17:09.316460Z","iopub.execute_input":"2025-01-23T18:17:09.316815Z","iopub.status.idle":"2025-01-23T18:17:09.320456Z","shell.execute_reply.started":"2025-01-23T18:17:09.316785Z","shell.execute_reply":"2025-01-23T18:17:09.319538Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"vtk_file_path = \"/kaggle/input/fine-tuning-lm-physical-interpretation-hackathon/Case1.vtk\"\nmesh = pv.read(vtk_file_path)\n\n# Extract Points (x, y, z)\npoints = np.array(mesh.points)\n\n# Extract Scalars (Point Data)\npoint_data = {name: mesh.point_data[name] for name in mesh.point_data.keys()}\n\n# Convert to DataFrame\ndf = pd.DataFrame(points, columns=[\"x\", \"y\", \"z\"])\n\n# Add scalar fields\nfor name, values in point_data.items():\n    df[name] = values\n\n# Save as CSV\ncsv_file_path = \"Case1vtk_extracted_data.csv\"\ndf.to_csv(csv_file_path, index=False)\n\nprint(f\"CSV file saved: {csv_file_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:17:16.916129Z","iopub.execute_input":"2025-01-23T18:17:16.916570Z","iopub.status.idle":"2025-01-23T18:17:17.010598Z","shell.execute_reply.started":"2025-01-23T18:17:16.916528Z","shell.execute_reply":"2025-01-23T18:17:17.009545Z"}},"outputs":[{"name":"stdout","text":"CSV file saved: Case1vtk_extracted_data.csv\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Load the VTK file\nvtk_file_path = \"/kaggle/input/fine-tuning-lm-physical-interpretation-hackathon/Case2.vtk\"\nmesh = pv.read(vtk_file_path)\n# Extract Points (x, y, z)\npoints = np.array(mesh.points)\n\n# Extract Scalars (Point Data)\npoint_data = {name: mesh.point_data[name] for name in mesh.point_data.keys()}\n\n# Convert to DataFrame\ndf = pd.DataFrame(points, columns=[\"x\", \"y\", \"z\"])\n\n# Add scalar fields\nfor name, values in point_data.items():\n    df[name] = values\n\n# Save as CSV\ncsv_file_path = \"Case2vtk_extracted_data.csv\"\ndf.to_csv(csv_file_path, index=False)\n\nprint(f\"CSV file saved: {csv_file_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:17:25.755291Z","iopub.execute_input":"2025-01-23T18:17:25.755576Z","iopub.status.idle":"2025-01-23T18:17:25.830986Z","shell.execute_reply.started":"2025-01-23T18:17:25.755555Z","shell.execute_reply":"2025-01-23T18:17:25.830052Z"}},"outputs":[{"name":"stdout","text":"CSV file saved: Case2vtk_extracted_data.csv\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Load the VTK file\nvtk_file_path = \"/kaggle/input/fine-tuning-lm-physical-interpretation-hackathon/Case3.vtk\"\nmesh = pv.read(vtk_file_path)\n# Extract Points (x, y, z)\npoints = np.array(mesh.points)\n\n# Extract Scalars (Point Data)\npoint_data = {name: mesh.point_data[name] for name in mesh.point_data.keys()}\n\n# Convert to DataFrame\ndf = pd.DataFrame(points, columns=[\"x\", \"y\", \"z\"])\n\n# Add scalar fields\nfor name, values in point_data.items():\n    df[name] = values\n\n# Save as CSV\ncsv_file_path = \"Case3vtk_extracted_data.csv\"\ndf.to_csv(csv_file_path, index=False)\n\nprint(f\"CSV file saved: {csv_file_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:17:32.221431Z","iopub.execute_input":"2025-01-23T18:17:32.221774Z","iopub.status.idle":"2025-01-23T18:17:32.329984Z","shell.execute_reply.started":"2025-01-23T18:17:32.221745Z","shell.execute_reply":"2025-01-23T18:17:32.329125Z"}},"outputs":[{"name":"stdout","text":"CSV file saved: Case3vtk_extracted_data.csv\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Load the VTK file\nvtk_file_path = \"/kaggle/input/fine-tuning-lm-physical-interpretation-hackathon/Case4.vtk\"\nmesh = pv.read(vtk_file_path)\n# Extract Points (x, y, z)\npoints = np.array(mesh.points)\n\n# Extract Scalars (Point Data)\npoint_data = {name: mesh.point_data[name] for name in mesh.point_data.keys()}\n\n# Convert to DataFrame\ndf = pd.DataFrame(points, columns=[\"x\", \"y\", \"z\"])\n\n# Add scalar fields\nfor name, values in point_data.items():\n    df[name] = values\n\n# Save as CSV\ncsv_file_path = \"Case4vtk_extracted_data.csv\"\ndf.to_csv(csv_file_path, index=False)\n\nprint(f\"CSV file saved: {csv_file_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:17:37.785631Z","iopub.execute_input":"2025-01-23T18:17:37.785985Z","iopub.status.idle":"2025-01-23T18:17:37.891930Z","shell.execute_reply.started":"2025-01-23T18:17:37.785960Z","shell.execute_reply":"2025-01-23T18:17:37.891016Z"}},"outputs":[{"name":"stdout","text":"CSV file saved: Case4vtk_extracted_data.csv\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 3. Data Preprocessing and Pipeline","metadata":{}},{"cell_type":"code","source":"csv_files = [\"/kaggle/working/Case1vtk_extracted_data.csv\",\n                \"/kaggle/working/Case2vtk_extracted_data.csv\",\n                \"/kaggle/working/Case3vtk_extracted_data.csv\",\n                \"/kaggle/working/Case4vtk_extracted_data.csv\"]\n# Merge all CSV files\nfinal_df = pd.concat([pd.read_csv(csv) for csv in csv_files], ignore_index=True)\n\n# Save the final merged dataset\nfinal_csv_path = \"/kaggle/working/final_vtk_dataset.csv\"\nfinal_df.to_csv(final_csv_path, index=False)\n\nprint(f\"Final dataset saved: {final_csv_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:17:43.651017Z","iopub.execute_input":"2025-01-23T18:17:43.651305Z","iopub.status.idle":"2025-01-23T18:17:43.866815Z","shell.execute_reply.started":"2025-01-23T18:17:43.651284Z","shell.execute_reply":"2025-01-23T18:17:43.865741Z"}},"outputs":[{"name":"stdout","text":"Final dataset saved: /kaggle/working/final_vtk_dataset.csv\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\ndata_path = \"/kaggle/working/final_vtk_dataset.csv\"  # Update with your dataset name\ndf = pd.read_csv(data_path)\n\n# Display the first few rows\nprint(df.head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:17:48.993840Z","iopub.execute_input":"2025-01-23T18:17:48.994149Z","iopub.status.idle":"2025-01-23T18:17:49.030823Z","shell.execute_reply.started":"2025-01-23T18:17:48.994122Z","shell.execute_reply":"2025-01-23T18:17:49.030005Z"}},"outputs":[{"name":"stdout","text":"     x         y    z  Temperature\n0  0.0  0.000000  0.0         -0.0\n1  0.0  0.010101  0.0         -0.0\n2  0.0  0.020202  0.0         -0.0\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(df.tail(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:17:54.491523Z","iopub.execute_input":"2025-01-23T18:17:54.491880Z","iopub.status.idle":"2025-01-23T18:17:54.498722Z","shell.execute_reply.started":"2025-01-23T18:17:54.491851Z","shell.execute_reply":"2025-01-23T18:17:54.497723Z"}},"outputs":[{"name":"stdout","text":"         x         y    z   Temperature\n39997  1.0  0.979798  0.0  7.037214e-18\n39998  1.0  0.989899  0.0  3.520380e-18\n39999  1.0  1.000000  0.0  1.358809e-32\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Create LLM fine-tuning dataset\ndf[\"prompt\"] = df.apply(lambda row: \n    f\"What is the temperature at point ({row['x']}, {row['y']}, {row['z']})?\", axis=1)\ndf[\"response\"] = df[\"Temperature\"].apply(lambda temp: f\"The temperature is {temp}.\")\n\n# Save the fine-tuning dataset\ndf[[\"prompt\", \"response\"]].to_csv(\"heat_equation_finetune.csv\", index=False)\nprint(\"Fine-tuning CSV saved: heat_equation_finetune.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:17:58.805317Z","iopub.execute_input":"2025-01-23T18:17:58.805689Z","iopub.status.idle":"2025-01-23T18:17:59.423394Z","shell.execute_reply.started":"2025-01-23T18:17:58.805626Z","shell.execute_reply":"2025-01-23T18:17:59.422470Z"}},"outputs":[{"name":"stdout","text":"Fine-tuning CSV saved: heat_equation_finetune.csv\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\ndata_path = \"/kaggle/working/heat_equation_finetune.csv\"  # Update with your dataset name\ndf = pd.read_csv(data_path)\n\n# Display the first few rows\nprint(df.head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:18:04.891990Z","iopub.execute_input":"2025-01-23T18:18:04.892289Z","iopub.status.idle":"2025-01-23T18:18:04.942983Z","shell.execute_reply.started":"2025-01-23T18:18:04.892261Z","shell.execute_reply":"2025-01-23T18:18:04.942309Z"}},"outputs":[{"name":"stdout","text":"                                              prompt                  response\n0  What is the temperature at point (0.0, 0.0, 0.0)?  The temperature is -0.0.\n1  What is the temperature at point (0.0, 0.01010...  The temperature is -0.0.\n2  What is the temperature at point (0.0, 0.02020...  The temperature is -0.0.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# consider only Case1 VTK file data:\ndef solve_heat_equation_case1():\n    nx, ny = 50, 50  \n    dx, dy = 1 / (nx - 1), 1 / (ny - 1) \n    x = np.linspace(0, 1, nx)\n    y = np.linspace(0, 1, ny)\n    X, Y = np.meshgrid(x, y)\n\n    T = np.zeros((ny, nx))\n\n    T[0, :] = 0  \n    T[-1, :] = 0 \n    T[:, 0] = 0  \n    T[:, -1] = 0 \n\n    def force_function(x, y):\n        return 8 * np.pi**2 * np.sin(2 * np.pi * x) * np.sin(2 * np.pi * y)\n\n    for _ in range(1000):  \n        T_new = T.copy()\n        for i in range(1, nx - 1):\n            for j in range(1, ny - 1):\n                T_new[j, i] = 0.25 * (T[j + 1, i] + T[j - 1, i] + T[j, i + 1] + T[j, i - 1] - dx**2 * force_function(x[i], y[j]))\n        T = T_new\n\n    grid = pv.StructuredGrid(X, Y, np.zeros_like(X))\n    grid[\"Temperature\"] = T.ravel()\n    grid.save(\"Case1_FDM.vtk\")\n\nsolve_heat_equation_case1()\n\ngrid = pv.read(\"Case1_FDM.vtk\")\n\n# Extract data\npoints = grid.points\ntemperature = grid[\"Temperature\"]\n\ndata = pd.DataFrame({\n    \"x\": points[:, 0],\n    \"y\": points[:, 1],\n    \"temperature\": temperature\n})\n\ndata.to_csv(\"Case1_FDM.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:18:10.804737Z","iopub.execute_input":"2025-01-23T18:18:10.805035Z","iopub.status.idle":"2025-01-23T18:18:19.253985Z","shell.execute_reply.started":"2025-01-23T18:18:10.805013Z","shell.execute_reply":"2025-01-23T18:18:19.253096Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## 4. Installing Required packages","metadata":{}},{"cell_type":"code","source":"!pip install transformers==4.46.0 datasets peft bitsandbytes accelerate torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:18:31.655426Z","iopub.execute_input":"2025-01-23T18:18:31.655795Z","iopub.status.idle":"2025-01-23T18:18:46.715050Z","shell.execute_reply.started":"2025-01-23T18:18:31.655762Z","shell.execute_reply":"2025-01-23T18:18:46.713968Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.46.0\n  Downloading transformers-4.46.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (0.4.5)\nCollecting tokenizers<0.21,>=0.20 (from transformers==4.46.0)\n  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.46.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.46.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.46.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.46.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.46.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.46.0) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0) (2024.12.14)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers==4.46.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers==4.46.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.46.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.46.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers==4.46.0) (2024.2.0)\n\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'transformers' candidate (version 4.46.0 at https://files.pythonhosted.org/packages/db/88/1ef8a624a33d7fe460a686b9e0194a7916320fc0d67d4e38e570beeac039/transformers-4.46.0-py3-none-any.whl (from https://pypi.org/simple/transformers/) (requires-python:>=3.8.0))\nReason for being yanked: This version unfortunately does not work with 3.8 but we did not drop the support yet\u001b[0m\u001b[33m\n\u001b[0mDownloading transformers-4.46.0-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers, bitsandbytes\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\nSuccessfully installed bitsandbytes-0.45.1 tokenizers-0.20.3 transformers-4.46.0\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Import necessary libraries\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorWithPadding\nfrom datasets import Dataset\nfrom peft import get_peft_model, TaskType\nfrom peft import LoraConfig, get_peft_model  # LoRA fine-tuning\nimport torch\nimport gc\nimport os\nfrom accelerate import Accelerator\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\naccelerator = Accelerator(cpu=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:19:00.241678Z","iopub.execute_input":"2025-01-23T18:19:00.242021Z","iopub.status.idle":"2025-01-23T18:19:21.307019Z","shell.execute_reply.started":"2025-01-23T18:19:00.241996Z","shell.execute_reply":"2025-01-23T18:19:21.306078Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## 5. Building LLM Pipeline","metadata":{}},{"cell_type":"code","source":"# Define model path\nmodel_path = \"ibm-granite/granite-3.1-8b-instruct\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:19:41.193224Z","iopub.execute_input":"2025-01-23T18:19:41.193519Z","iopub.status.idle":"2025-01-23T18:19:41.197262Z","shell.execute_reply.started":"2025-01-23T18:19:41.193497Z","shell.execute_reply":"2025-01-23T18:19:41.196379Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# ✅ Auto-detect the best available device (GPU preferred)\n#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T04:00:00.560325Z","iopub.execute_input":"2025-01-23T04:00:00.560770Z","iopub.status.idle":"2025-01-23T04:00:00.565378Z","shell.execute_reply.started":"2025-01-23T04:00:00.560736Z","shell.execute_reply":"2025-01-23T04:00:00.564205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.pad_token = tokenizer.eos_token  # Set padding token to avoid errors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:20:07.052175Z","iopub.execute_input":"2025-01-23T18:20:07.052492Z","iopub.status.idle":"2025-01-23T18:20:08.313390Z","shell.execute_reply.started":"2025-01-23T18:20:07.052461Z","shell.execute_reply":"2025-01-23T18:20:08.312732Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e16c3f8522148318b924db2c4ba5f63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9edee3ad54e34f1fa775e436962dff45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a5d1b262a234e02ab5e54c08eb961d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9f7628b9c5744e2b6b62f81d944c565"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"766343f7cbef47ff91fc16b4035fc04d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/701 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4ae475a4da54724addc77a3ca58af33"}},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# ✅ Load model with 4-bit quantization to save memory\nmodel = AutoModelForCausalLM.from_pretrained(model_path, \n                                             device_map=\"auto\", \n                                             load_in_4bit=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:20:18.417438Z","iopub.execute_input":"2025-01-23T18:20:18.417767Z","iopub.status.idle":"2025-01-23T18:27:40.571082Z","shell.execute_reply.started":"2025-01-23T18:20:18.417741Z","shell.execute_reply":"2025-01-23T18:27:40.570252Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/790 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa4590b2566b44cca6cdb8697002c6fd"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/29.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1ea2fc9c85140b3a1d897a861073dc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89c8a0c15c8641579929626cc3fa8f26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea7087d7b45343b897df26cbf9ad4e37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e9a4dc15b274e758cbc9f4fde1db496"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"814fc498b56d49f5b356108656f1d600"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.41G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89e097b73b134ac7a4e3220b49ae0350"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb93b03422a7498a84dcd7d7afeb767c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bc0f676fbd748a88dd2f12409e465c7"}},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"model.gradient_checkpointing_enable()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:28:28.889130Z","iopub.execute_input":"2025-01-23T18:28:28.889437Z","iopub.status.idle":"2025-01-23T18:28:28.895197Z","shell.execute_reply.started":"2025-01-23T18:28:28.889414Z","shell.execute_reply":"2025-01-23T18:28:28.894300Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# ✅ Apply LoRA (Low-Rank Adaptation) to speed up fine-tuning\nlora_config = LoraConfig(\n    r=4,  # Low-rank adaptation size\n    lora_alpha=16,  # Scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],  # LoRA applied to attention layers\n    lora_dropout=0.01,\n    task_type=TaskType.CAUSAL_LM)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:28:45.254047Z","iopub.execute_input":"2025-01-23T18:28:45.254361Z","iopub.status.idle":"2025-01-23T18:28:45.258399Z","shell.execute_reply.started":"2025-01-23T18:28:45.254339Z","shell.execute_reply":"2025-01-23T18:28:45.257583Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"model = get_peft_model(model, lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:28:51.649203Z","iopub.execute_input":"2025-01-23T18:28:51.649589Z","iopub.status.idle":"2025-01-23T18:28:51.779539Z","shell.execute_reply.started":"2025-01-23T18:28:51.649557Z","shell.execute_reply":"2025-01-23T18:28:51.778677Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# solution for case1 data\ndata = pd.read_csv(\"Case1_FDM.csv\")\ndataset = Dataset.from_pandas(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:28:57.166728Z","iopub.execute_input":"2025-01-23T18:28:57.167067Z","iopub.status.idle":"2025-01-23T18:28:57.197674Z","shell.execute_reply.started":"2025-01-23T18:28:57.167045Z","shell.execute_reply":"2025-01-23T18:28:57.196850Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Load dataset\n#dataset = load_dataset(\"csv\", data_files=\"heat_equation_finetune.csv\", split=\"train[:3%]\")\n#print((dataset[120]))  # Prints the number of rows in the dataset\n#print(len(dataset))  # Prints the number of rows in the dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T04:02:37.452424Z","iopub.execute_input":"2025-01-23T04:02:37.452743Z","iopub.status.idle":"2025-01-23T04:02:37.729793Z","shell.execute_reply.started":"2025-01-23T04:02:37.452716Z","shell.execute_reply":"2025-01-23T04:02:37.729164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Tokenization function\ndef preprocess_function(examples):\n    examples[\"temperature\"] = [str(temp) for temp in examples[\"temperature\"]]\n    tokenized_inputs = tokenizer(examples[\"temperature\"], truncation=True, max_length=512)\n    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n\n    return tokenized_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:29:03.859548Z","iopub.execute_input":"2025-01-23T18:29:03.859948Z","iopub.status.idle":"2025-01-23T18:29:03.864456Z","shell.execute_reply.started":"2025-01-23T18:29:03.859917Z","shell.execute_reply":"2025-01-23T18:29:03.863632Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"tokenized_dataset = dataset.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:29:10.498421Z","iopub.execute_input":"2025-01-23T18:29:10.498816Z","iopub.status.idle":"2025-01-23T18:29:10.650526Z","shell.execute_reply.started":"2025-01-23T18:29:10.498783Z","shell.execute_reply":"2025-01-23T18:29:10.649617Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9936695910e4e409601579eb4a2497b"}},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:29:15.929834Z","iopub.execute_input":"2025-01-23T18:29:15.930144Z","iopub.status.idle":"2025-01-23T18:29:16.269418Z","shell.execute_reply.started":"2025-01-23T18:29:15.930124Z","shell.execute_reply":"2025-01-23T18:29:16.268633Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"91"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:29:22.252961Z","iopub.execute_input":"2025-01-23T18:29:22.253295Z","iopub.status.idle":"2025-01-23T18:29:22.257105Z","shell.execute_reply.started":"2025-01-23T18:29:22.253266Z","shell.execute_reply":"2025-01-23T18:29:22.256195Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"## 6. Generate training and validation datasets","metadata":{}},{"cell_type":"code","source":"#✅ Shuffle once and split into 80% train / 20% eval\nsplit_idx = int(0.8 * len(tokenized_dataset))\ntrain_data = tokenized_dataset.shuffle(seed=42).select(range(split_idx))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T04:03:04.261922Z","iopub.execute_input":"2025-01-23T04:03:04.262317Z","iopub.status.idle":"2025-01-23T04:03:04.273542Z","shell.execute_reply.started":"2025-01-23T04:03:04.262288Z","shell.execute_reply":"2025-01-23T04:03:04.272594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_data = tokenized_dataset.select(range(split_idx, len(tokenized_dataset)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T04:03:08.639803Z","iopub.execute_input":"2025-01-23T04:03:08.640095Z","iopub.status.idle":"2025-01-23T04:03:08.647136Z","shell.execute_reply.started":"2025-01-23T04:03:08.640073Z","shell.execute_reply":"2025-01-23T04:03:08.646282Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Implement a fine-tuning methodology-using the prepared datasets","metadata":{}},{"cell_type":"code","source":"# ✅ Training arguments optimized for speed\ntraining_args = TrainingArguments(\n    output_dir=\"./granite_finetuned\",\n    run_name=\"granite_experiment\",\n    per_device_train_batch_size=1,  # Increase batch size if GPU allows\n    gradient_accumulation_steps=8,  # ✅ Simulates larger batch size\n    #per_device_eval_batch_size=8,\n    num_train_epochs=3,  # Reduce training time\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    fp16=True,   # ✅ Mixed precision training (Faster training)\n    save_strategy=\"epoch\", # Save model every epoch \n    logging_dir=\"./logs\",  \n    logging_steps=10,# Log every 10 steps\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:29:32.106278Z","iopub.execute_input":"2025-01-23T18:29:32.106587Z","iopub.status.idle":"2025-01-23T18:29:32.141248Z","shell.execute_reply.started":"2025-01-23T18:29:32.106564Z","shell.execute_reply":"2025-01-23T18:29:32.140588Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# ✅ Initialize Trainer with optimized settings\nfrom transformers import Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    data_collator=data_collator,\n    #eval_dataset=eval_data\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:29:39.833999Z","iopub.execute_input":"2025-01-23T18:29:39.834301Z","iopub.status.idle":"2025-01-23T18:29:41.759342Z","shell.execute_reply.started":"2025-01-23T18:29:39.834277Z","shell.execute_reply":"2025-01-23T18:29:41.758720Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"model, trainer = accelerator.prepare(model, trainer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T18:29:47.187219Z","iopub.execute_input":"2025-01-23T18:29:47.187599Z","iopub.status.idle":"2025-01-23T18:29:47.193728Z","shell.execute_reply.started":"2025-01-23T18:29:47.187555Z","shell.execute_reply":"2025-01-23T18:29:47.192708Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# ✅ Train the model (Now much faster!)\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Save fine-tuned model & tokenizer\nmodel.save_pretrained(\"./fine_tuned_model\")\ntokenizer.save_pretrained(\"./fine_tuned_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# 🚀 Model Inference Function","metadata":{}},{"cell_type":"code","source":"def load_model():\n    \"\"\"Load fine-tuned model for inference.\"\"\"\n    model_path = \"fine_tuned_model\"\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n    model.eval()\n    return model, tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model_response(model, tokenizer, prompt, max_tokens=300):\n    \"\"\"Generate response from fine-tuned model.\"\"\"\n    input_text = f\"Question: {prompt}\\n\\nAnswer:\"\n    input_tokens = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    \n    output = model.generate(\n        **input_tokens,\n        max_new_tokens=max_tokens,\n        temperature=0.7,\n        top_p=0.95,\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id\n    )\n     # Decode and clean up the response\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\n    # Remove the input prompt from the response\n    response = response[len(input_text):].strip()\n    return response","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def model_inference():\n    \"\"\"Run inference for all cases.\"\"\"\n    model, tokenizer = load_model()\n    \n    cases = {\n        \"Case1Q1\": \"What is the temperature distribution at the corner (0,0) of the unit square mesh?\",\n        \"Case1Q2\": \"How does the temperature change with respect to x-axis at y=0.5?\",\n        \"Case1Q3\": \"If we increase the coefficient of pi in the force function, what will happen?\",\n        \"Case2Q1\": \"Explain why the temperature is zero at both x=0 and x=1, and what this means physically.\",\n        \"Case2Q2\": \"At what coordinates does the maximum temperature occur, and what determines this location?\",\n        \"Case2Q3\": \"How does the temperature profile change along x=0.5 compared to x=0.25?\",\n        \"Case3Q1\": \"What is the temperature at the corner (0,0) of the unit square mesh?\",\n        \"Case3Q2\": \"What physical significance does the boundary condition u(0,y)=0 have?\",\n        \"Case3Q3\": \"What does the boundary condition u(1,y)=y(1−y) represent physically?\",\n        \"Case4Q1\": \"What can you infer about the decay rate of temperature?\",\n        \"Case4Q2\": \"Why does the spatial pattern remain unchanged while only the amplitude decreases with time?\",\n        \"Case4Q3\": \"What is the effect of alpha on the decay rate of heat dissipation?\"\n    }\n    \n    results = []\n    for case, prompt in cases.items():\n        print(f\"\\nTesting {case}...\")\n        try:\n            response = get_model_response(model, tokenizer, prompt)\n            results.append(response)\n            print(f\"\\nResponse for {case}:\")\n            print(response)\n        except Exception as e:\n            print(f\"Error in {case}: {str(e)}\")\n            results.append(f\"Error: {str(e)}\")\n    \n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🚀 Run Inference and Save Results","metadata":{}},{"cell_type":"code","source":"# ✅ Save results as CSV\ndf = pd.DataFrame({\"Id\": list(range(1, 13)), \"Answer\": results})\ndf.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🚀 Evaluation Metrics (BLEU & ROUGE)","metadata":{}},{"cell_type":"code","source":"!pip install sacrebleu rouge-score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Load BLEU and ROUGE metrics\nbleu_metric = load_metric(\"sacrebleu\")\nrouge_metric = load_metric(\"rouge\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_metric","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Load BLEU and ROUGE metrics\nbleu_metric = load_metric(\"sacrebleu\")\nrouge_metric = load_metric(\"rouge\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reference_answers = [\n    \"The temperature at (0,0) is 0 degrees.\",\n    \"The temperature along x-axis at y=0.5 increases quadratically, ranging from 0.25 to 1.25.\",\n    \"Increasing the coefficient of π results in higher overall temperatures throughout the domain.\",\n    \"The temperature is zero at x=0 and x=1 because of Dirichlet boundary conditions, meaning the boundaries are held at a fixed temperature.\",\n    \"The maximum temperature occurs at (1,1) because it is the highest sum of x² + y².\",\n    \"The temperature profile at x=0.5 is symmetric about y=0.5, with a parabolic variation.\",\n    \"The temperature at (0,0) is 0 degrees.\",\n    \"The boundary condition u(0,y)=0 ensures no heat flux at the left boundary.\",\n    \"The boundary condition u(1,y)=y(1−y) represents a parabolic temperature distribution at x=1.\",\n    \"The temperature decay rate follows an exponential pattern due to the heat dissipation properties of the material.\",\n    \"The spatial pattern remains unchanged while amplitude decreases because the system reaches a self-similar equilibrium.\",\n    \"Increasing alpha accelerates heat dissipation, leading to a faster decay in temperature.\"\n]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Convert responses into evaluation format\npredictions = results  # Model-generated responses\nreferences = [[ref] for ref in reference_answers]  # Convert to required format\n\n# ✅ Compute BLEU and ROUGE Scores\nbleu_score = bleu_metric.compute(predictions=predictions, references=references)\nrouge_score = rouge_metric.compute(predictions=predictions, references=references)\n\n# ✅ Print evaluation results\nprint(f\"\\nBLEU Score: {bleu_score['score']:.2f}\")\nprint(f\"ROUGE-L Score: {rouge_score['rougeL'].mid.fmeasure:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🚀 Reporting File","metadata":{}},{"cell_type":"markdown","source":"#### https://github.com/sajibhalder/FineTune-LLM-Heat-Equation-Solution","metadata":{}}]}